{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYYWchAQmML8Usfx96LaQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Signed-B/build-your-own-llm/blob/main/lecture_workbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How To Build a Large Language Model\n",
        "### Presented by Beckett Hyde - 03/05/2024\n",
        "\n",
        "Welcome to this notebook that walks you through all the steps necessary for creating your first LLM application:\n",
        "+ Pulling pre-trained base model weights\n",
        "+ Loading those weights into memory, incuding on small, consumer GPUs\n",
        "+ Pulling and tokenizing a dataset (or tokenizing your own dataset)\n",
        "+ Fine-tuning the model\n",
        "+ Model inference\n",
        "\n",
        "This is designed to go along with the in-person presentation given on March 5th to the University of Colorado, Boulder. This does not cover training a model from scratch (as that is extremely time and resource intensive) and takes extra steps to ensure the model can even work on higher-end consumer hardware (no NVIDIA A100 GPUs required! If your machine has ~15GB of vRAM, sometimes even less, this could work off of colab's T4s for you).\n",
        "\n",
        "If you are interested in skipping straight to the working solutions, see [this notebook](https://github.com/Signed-B/build-your-own-llm/blob/main/lecture_solution.ipynb). If you are interested in the theory portion of the presetation, see [this sldieshow](example.com).\n",
        "\n",
        "<hr />\n",
        "\n",
        "This presentation was sponsored by the CU Boulder Undergraduate SIAM Chapter. A thank you to them for their support in promoting and financing the event.\n",
        "\n",
        "<center>\n",
        "<p float=\"left\">\n",
        "  <img src=\"https://www.colorado.edu/brand/sites/default/files/styles/medium/public/block/boulder-one-line_4.png\" width=\"300\" />\n",
        "  <img width=\"10\" hspace=\"10\" />\n",
        "  <img src=\"https://www.siam.org/portals/0/Logo%20Guide/logo_cobrand.png\" width=\"300\" />\n",
        "</p>\n",
        "</center>\n",
        "<hr/>\n",
        "<hr />\n",
        "\n",
        "# Step 1: Install Software\n",
        "\n",
        "A number of packages, some with specific versions, are required for this notebook to work (especially with the modifications we make for smaller hardware). For simplicity, we give these to you. Run the below cell to prepare your environment.\n",
        "\n",
        "### A note about Google Colab:\n",
        "\n",
        "Ensure you are running on Colab's T4 environment, which is available as part of the free tier.\n",
        "\n",
        "It is also possibe that you may have to disconnect and reconnect to your environment for some of these installs to take effect (particularly `accelerate`)."
      ],
      "metadata": {
        "id": "gRkcv2q0zAbb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "odquqWeHy38u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf3336ec-6ed2-49f6-b4e5-b16372c1d368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.21.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.34.0\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Download & configure the base model\n",
        "\n",
        "Almost all interesting pre-trained models (both base-models are available on [Huggingface](huggingface.co). We will be downloading a specific version of the `MPT-7B` model, released in July 2018, with modifications to work on our limited hardware.\n",
        "\n",
        "The model is `eluzhnica/mpt-7b-8k-peft-compatible` available [here](https://huggingface.co/eluzhnica/mpt-7b-8k-peft-compatible)."
      ],
      "metadata": {
        "id": "5rkNNqc_6U_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"eluzhnica/mpt-7b-8k-peft-compatible\"\n"
      ],
      "metadata": {
        "id": "aurC-vwC6Tai"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order for our model to work, we need to use gradient checkpointing and \"reformat\" the model to work with our artificially reduced bit-sizes."
      ],
      "metadata": {
        "id": "PeKVanvlDPN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n"
      ],
      "metadata": {
        "id": "5iGcZQxeC03r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How big is our model? How many parameters are trainable?"
      ],
      "metadata": {
        "id": "W9LYb1B7DuKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"All params:\", sum([param.numel() for _, param in model.named_parameters()]))\n",
        "# print(\"Trainable params:\", sum([param.numel() if param.requires_grad else 0 for _, param in model.named_parameters()]))"
      ],
      "metadata": {
        "id": "qI-CfGp6Dv5a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prepare our model for training\n",
        "\n",
        "Our model currently has no trainable paramaters. We're going to use LoRA as a training algorithm (using `peft`) with a lot of hyperparameters to control how we train.\n",
        "\n",
        "### What does each hyperparameter do?\n",
        "\n",
        "TODO"
      ],
      "metadata": {
        "id": "BrW3miHxFTPh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ebjeLSDPFcaZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many parameters are trainable now?"
      ],
      "metadata": {
        "id": "dlquxgXmGCy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"All params:\", sum([param.numel() for _, param in model.named_parameters()]))\n",
        "# print(\"Trainable params:\", sum([param.numel() if param.requires_grad else 0 for _, param in model.named_parameters()]))"
      ],
      "metadata": {
        "id": "gtq4Mc05GCgy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load and tokenize our data\n",
        "\n",
        "Huggingface, like Kaggle, also holds publicly available datasets we can use, like `vicgalle/alpaca-gpt4`, which was actually created, [ironically](https://en.wikipedia.org/wiki/Dead_Internet_theory), using GPT-4!\n",
        "\n",
        "Each model comes with a pre-trained tokenizer we can use. This converts the text into a series of tokens represented in a high-dimensional space where relative proximity encodes meaning. We use the `datasets` library to manage our dataseets and the `transformers.AutoTokenizer` class to pull and hold our tokenizer."
      ],
      "metadata": {
        "id": "MOiuu_W4DpVK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K6EQg_8gGr5A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's explore the data!\n",
        "\n",
        "You aren't like those *other* data scientists and MLEs, you actually *do your job!* (please god I am done fixing your messes)."
      ],
      "metadata": {
        "id": "VXOwo4beHcYu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGhVVRBdHZzu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train test splits"
      ],
      "metadata": {
        "id": "q1sW4V89IlI-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQw3kNRwIqrm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3.5: Test the base model\n",
        "\n",
        "Warning: the model is completely untrained and simply predicts the most likely next token. It is very easy to get it to \"say\" unsavory things at this stage."
      ],
      "metadata": {
        "id": "J3XGaTUKLWgt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5bIBzkNZLhUF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it with one of our prompts."
      ],
      "metadata": {
        "id": "dgcuv81cMNPd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKJ0TgyrMfIl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Generate a creative activity for a child to do during their summer vacation.\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "print(prompt)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVaBh5jzMMkM",
        "outputId": "aa3fd68e-3cef-4ba2-fdb0-9fc8bb53bb2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a creative activity for a child to do during their summer vacation.\n",
            "\n",
            "### Response:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Train the model!\n",
        "\n",
        "Using the full power of `transformers` now, we create a training plan with `TrainingArguments` and a trainer with `Trainer`.\n",
        "\n",
        "We use the following settings:\n",
        "TODO"
      ],
      "metadata": {
        "id": "JJPkeUDUJnHF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b3lWYTyIJkqu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train!"
      ],
      "metadata": {
        "id": "zLnyZAemLPy9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9431Z1pfLFye"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Inference\n",
        "\n",
        "Now we use the model to answer some questions."
      ],
      "metadata": {
        "id": "m4DdDDpVOA8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "def stream(question, context=None):\n",
        "    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
        "    inst_tag, input_tag, resp_tag = \"### Instruction:\\n\", \"### Input:\\n\", \"### Response:\\n\"\n",
        "\n",
        "    prompt = f\"{system_prompt}{inst_tag}{question.strip()}\\n\\n{input_tag}{context.strip()}\\n\\n{resp_tag}\" \\\n",
        "             if context else f\"{system_prompt}{inst_tag}{question.strip()}\\n\\n{resp_tag}\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
        "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=50)"
      ],
      "metadata": {
        "id": "AN0qntznNLEy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tHNoJsVOWns"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js1tmIuSQcxR"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}